<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects on Phil in the Blank</title><link>/projects/</link><description>Recent content in Projects on Phil in the Blank</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 19 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="/projects/index.xml" rel="self" type="application/rss+xml"/><item><title>Predicting mental health and substance use in adulthood from resilience in adolescence</title><link>/projects/resilience-imagen/</link><pubDate>Mon, 19 Dec 2022 00:00:00 +0000</pubDate><guid>/projects/resilience-imagen/</guid><description>&lt;h2 id="project-summary-bayes-mediation-lots-of-data">Project Summary: Bayes. Mediation. Lots of data.&lt;/h2>
&lt;p>In this study I use data from the &lt;a href="https://imagen-project.org/">IMAGEN Study&lt;/a> (initial n=2315) to determine if mental health and drug use in adulthood (~age 23) can be predicted by resilience in early adolescence (~age 14). Adolescents are classified into one of four groups following &lt;a href="https://pubmed.ncbi.nlm.nih.gov/27079174">Burt et al’s&lt;/a> operationalization of resilience as the interaction between competence and adversity. In addition, I look at whether structural brain differences at age 14 in brain regions identified by Burt mediate any of the differences observed in mental health outcomes and alcohol use at age 23.&lt;/p>
&lt;p>To summarize, I find that there are significant differences in mental health outcomes and recent drug use at age 23 between the four groups. Additionally, the gray matter volume differences identified between the groups at age 14 did not mediate any of the differences in mental health or drug use. These results suggest that developmental features of early resilience such as social skills interact with adversity in a positive way and are predictive of numerous outcomes in adulthood. This corroborates with findings by (Roisman et al. 2004) that suggest the importance of developmentally salient tasks in predicting adult success.&lt;/p>
&lt;p>View the rest of the interact report &lt;a href="https://nguyenhphilip.github.io/resilience-imagen/">here&lt;/a>.&lt;/p>
&lt;p>&lt;img src="/images/projects/tds-contrast-1.png" alt="">&lt;/p></description></item><item><title>Exploring Mental Health Networks</title><link>/projects/pocs-mh-nets/</link><pubDate>Tue, 28 Dec 2021 00:00:00 +0000</pubDate><guid>/projects/pocs-mh-nets/</guid><description>&lt;p>This is a brief project where I look at mental health symptom networks in a large sample of youth from the &lt;a href="https://abcdstudy.org/">ABCD Study&lt;/a> for the course &lt;a href="https://pdodds.w3.uvm.edu/teaching/courses/2021-2022principles-of-complex-systems/">Principles of Complex Systems&lt;/a>. I was largely inspired by the work being done by &lt;a href="https://onlinelibrary.wiley.com/doi/full/10.1002/wps.20375">Denny Borsboom&lt;/a> and &lt;a href="http://psychosystems.org/NetworkSchool">company&lt;/a> using network analysis to study mental health. There is still a lot of work to do, but the report for the class can be found &lt;a href="https://github.com/nguyenhphilip/pocs-mh-nets/blob/main/CSYS_Mental_Health_Networks.pdf">here&lt;/a>.&lt;/p>
&lt;p>Check out &lt;a href="https://psyarxiv.com/f68ej/">this awesome paper&lt;/a> for more on applying complex systems thinking to psychopathology!&lt;/p>
&lt;p>&lt;img src="/images/projects/mh-net1.png" alt="">&lt;/p></description></item><item><title>Bayesian PCA</title><link>/projects/bayesian-pca/</link><pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate><guid>/projects/bayesian-pca/</guid><description>&lt;p>This is a project I worked on for my Bayesian Statistics course. My partner and I explored Bayesian Principal Component Analysis (BPCA) as an extension of PCA and PPCA on MNIST and an ecological dataset. I learned quite a bit about PCA in the process! The final report can be found &lt;a href="https://github.com/nguyenhphilip/stat330project/blob/main/STAT_330_Preprint_draft.pdf">here&lt;/a>.&lt;/p>
&lt;p>&lt;img src="/images/projects/bpca1.png" alt="">&lt;/p>
&lt;p>&lt;img src="/images/projects/bpca2.png" alt="">&lt;/p></description></item><item><title>Character Space &amp; TV Tropes</title><link>/projects/character-space/</link><pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate><guid>/projects/character-space/</guid><description>&lt;p>This is an exploratory project I worked on with another graduate student in Data Science II. We explore the space of personality in fictional characters using data from &lt;a href="https://openpsychometrics.org/tests/characters/">OpenPsychometrics&lt;/a>. The bulk of the project was spent on probing and describing the space using dimensionality reduction with SVD. A link to the full work is &lt;a href="CSYS_387_Final_Paper.pdf">here&lt;/a>.&lt;/p>
&lt;p>&lt;img src="/images/projects/cspace1.png" alt="">&lt;/p>
&lt;p>&lt;img src="/images/projects/cspace2.png" alt="">&lt;/p></description></item><item><title>Spotify Top 100 Songs Analysis</title><link>/projects/spotify-top100/</link><pubDate>Sat, 19 Dec 2020 00:00:00 +0000</pubDate><guid>/projects/spotify-top100/</guid><description>&lt;p>I&amp;rsquo;ve been an avid Spotify user since my younger high-school gaming days (hipster flex), but Spotify only recently started releasing yearly music reviews that summarize the music you&amp;rsquo;ve listened to throughout that year. After seeing mine for 2020, I wondered how much my music taste had changed over time, especially given the 2020 coronavirus pandemic. Thankfully there&amp;rsquo;s a R package called &lt;code>spotifyr&lt;/code> that provides intuitive wrapper functions that allows one to analyze practically the entirety of Spotify music data! What follows is a quick, informal dive into this. (For getting setup, I suggest checking out the Github repo &lt;a href="https://github.com/charlie86/spotifyr">here&lt;/a> and this tutorial &lt;a href="https://msmith7161.github.io/what-is-speechiness/">here&lt;/a>.)&lt;/p>
&lt;h1 id="the-data-and-song-features">The Data and Song Features&lt;/h1>
&lt;p>To answer my question, I&amp;rsquo;m using my &amp;ldquo;Top 100 songs of the Year&amp;rdquo; from 2016-2020 (500 songs), looking specifically at nine features associated with each song (pretty neat that Spotify does this!). I knew I&amp;rsquo;d been listening to more instrumental, acoustic music since I started learning to play guitar, so I was curious about what the data showed. The features, from their &lt;a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/">API&lt;/a>:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>acousticness&lt;/strong>: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>danceability&lt;/strong>: How suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>energy&lt;/strong>: A measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>instrumentalness&lt;/strong>: Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>liveness&lt;/strong>: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>loudness&lt;/strong>: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>speechiness&lt;/strong>: Detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>tempo&lt;/strong>: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>valence&lt;/strong>: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="uh-oh">Uh Oh&lt;/h1>
&lt;p>Wellp, looking at the different facets, looks like acousticness and instrumentalness has increased, just as suspected. But valence (positive emotion) has decreased, hitting a low in 2020! Whether or not I was actually sadder on average each day of 2020 is another story&amp;hellip;&lt;/p>
&lt;p>&lt;img src="/images/projects/pressure-1.png" alt="">&lt;/p>
&lt;p>Here&amp;rsquo;s a closer look via some bar plots. The general trend is that everything has decreased for each feature except acousticness and instrumentalness. Loudness has decreased though, which maybe just means I&amp;rsquo;m getting older&amp;hellip; or more sensitive to loud music. But it makes sense if you think about the increase in acousticness.&lt;/p>
&lt;p>&lt;img src="/images/projects/barplot-1.png" alt="">&lt;/p>
&lt;p>And numbers. Voilà, une table.&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>Features&lt;/th>
 &lt;th>2016&lt;/th>
 &lt;th>2017&lt;/th>
 &lt;th>2018&lt;/th>
 &lt;th>2019&lt;/th>
 &lt;th>2020&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>acousticness&lt;/td>
 &lt;td>0.24&lt;/td>
 &lt;td>0.28&lt;/td>
 &lt;td>0.39&lt;/td>
 &lt;td>0.48&lt;/td>
 &lt;td>0.56&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>danceability&lt;/td>
 &lt;td>0.59&lt;/td>
 &lt;td>0.62&lt;/td>
 &lt;td>0.56&lt;/td>
 &lt;td>0.53&lt;/td>
 &lt;td>0.49&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>energy&lt;/td>
 &lt;td>0.63&lt;/td>
 &lt;td>0.62&lt;/td>
 &lt;td>0.52&lt;/td>
 &lt;td>0.49&lt;/td>
 &lt;td>0.42&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>instrumentalness&lt;/td>
 &lt;td>0.12&lt;/td>
 &lt;td>0.11&lt;/td>
 &lt;td>0.18&lt;/td>
 &lt;td>0.24&lt;/td>
 &lt;td>0.52&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>liveness&lt;/td>
 &lt;td>0.18&lt;/td>
 &lt;td>0.15&lt;/td>
 &lt;td>0.17&lt;/td>
 &lt;td>0.15&lt;/td>
 &lt;td>0.14&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>loudness&lt;/td>
 &lt;td>-7.41&lt;/td>
 &lt;td>-8.07&lt;/td>
 &lt;td>-10.11&lt;/td>
 &lt;td>-11.85&lt;/td>
 &lt;td>-13.37&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>speechiness&lt;/td>
 &lt;td>0.09&lt;/td>
 &lt;td>0.07&lt;/td>
 &lt;td>0.06&lt;/td>
 &lt;td>0.04&lt;/td>
 &lt;td>0.04&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>tempo&lt;/td>
 &lt;td>122.08&lt;/td>
 &lt;td>123.69&lt;/td>
 &lt;td>121.12&lt;/td>
 &lt;td>121.26&lt;/td>
 &lt;td>116.32&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>valence&lt;/td>
 &lt;td>0.45&lt;/td>
 &lt;td>0.48&lt;/td>
 &lt;td>0.41&lt;/td>
 &lt;td>0.42&lt;/td>
 &lt;td>0.31&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h1 id="inconclusive-conclusions">Inconclusive Conclusions&lt;/h1>
&lt;p>I&amp;rsquo;d say from personal experience that this trend toward increased acousticness and instrumentalness is a general feature of widened musical taste. I imagine it&amp;rsquo;s also associated with more bouts of work and study sessions, especially in 2020 given I started graduate school. But I also discovered more music of an acoustic, instrumental nature (Olafur Arnalds, City of the Sun, Max Richter) which I played HEAVILY on repeat while slacklining and rollerblading, two new hobbies I picked up over the summer. My love of fingerpicking on guitar probably has an influence on that too.&lt;/p>
&lt;p>As for the valence, well, let&amp;rsquo;s find out next year if that changes. I don&amp;rsquo;t actually feel like my subjective experience of days are all too different from the previous years, although some days are definitely lonelier. In moments like those I often turn to music. What the data doesn&amp;rsquo;t capture is the upswing in mood I feel from that form of connection. And don&amp;rsquo;t worry, I&amp;rsquo;m still dancing in my kitchen.&lt;/p></description></item><item><title>Data Science I Final Project: TV Tropes</title><link>/projects/tv-tropes/</link><pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate><guid>/projects/tv-tropes/</guid><description>&lt;p>This is a project I worked on with another graduate student in Data Science I. We open-sourced the code here: &lt;a href="https://github.com/jwzimmer/tv-tropes">TV Tropes Data Science I Project&lt;/a>. The project involved web scraping a large chunk of webpages from TV Tropes, network analysis, and data science!&lt;/p>
&lt;p>&lt;img src="/images/projects/top_100nodes_6communities.png" alt="">&lt;/p></description></item><item><title>Life After: A #NET-COVID Survey of Beliefs</title><link>/projects/life-after/</link><pubDate>Mon, 27 Apr 2020 00:00:00 +0000</pubDate><guid>/projects/life-after/</guid><description>&lt;p>The &lt;a href="https://sites.google.com/view/life-after-covid19/english">Life After&lt;/a> project is a survey of beliefs about what people believe the world will look like after the COVID-19 pandemic ends. I created &lt;a href="https://phil-nguyen.shinyapps.io/life-after/">an interactive website&lt;/a> to show the results of our analyses using R and Shiny.&lt;/p>
&lt;p>The motivation:&lt;/p>
&lt;blockquote>
&lt;p>COVID-19 is one of the largest pandemics of the century. It will have lasting effects on society around the globe. There has been a great deal of speculation about what those effects may be. But, there has yet to be an assessment of what individuals believe the effects will be. This study aims to create a picture of the world after the pandemic, from the perspectives of individuals across the globe. The survey collects anonymous information about socio-demographic and individual characteristics, and asks questions regarding your beliefs about the future behaviors of yourself and others.&lt;/p>
&lt;/blockquote>
&lt;p>This project was created as part of the &lt;a href="http://www.combine.umd.edu/network-epidemiology/">Network Epidemiology workshop&lt;/a> presented by the University of Maryland COMBINE program in collaboration with the University of Vermont Complex Systems Center. We had two weeks to design the study, collect responses, and analyze the data.&lt;/p>
&lt;p>All in all, it was a fun project and I learned a lot about Shiny app development, making GIFs, and survey research in general. I am most grateful for the friendships I have developed out of this. Thanks Sam, José, and Rory!&lt;/p>
&lt;p>&lt;img src="/images/projects/life-after.png" alt="">&lt;/p></description></item><item><title>Prematurity, Mental Health, and Cognition</title><link>/projects/prematurity-analysis/</link><pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate><guid>/projects/prematurity-analysis/</guid><description>&lt;p>This is a project examining the effect of premature birth on mental health and neurocognition in a large sample of youth from the &lt;a href="abcdstudy.org">ABCD Study&lt;/a>.&lt;/p>
&lt;p>Below is an interactive HTML page using R Markdown for the analyses and visualization. The source code is posted &lt;a href="https://github.com/nguyenhphilip/interactive_analysis_prem">here&lt;/a> and there is an interactive Rmarkdown page &lt;a href="premat">here&lt;/a> as well.&lt;/p>
&lt;p>&lt;img src="/images/projects/premat-bruno-img.png" alt="">&lt;/p></description></item><item><title>Intro to R Workshop</title><link>/projects/intro-r-workshop/</link><pubDate>Wed, 10 Jul 2019 00:00:00 +0000</pubDate><guid>/projects/intro-r-workshop/</guid><description>&lt;p>These are some slides for an introductory R and Tidyverse workshop I lead for undergraduates and graduate students working in our lab. We ran 4 workshops total:&lt;/p>
&lt;ul>
&lt;li>Day 1: Setup R, RStudio, motivations for learning R&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/presentation/d/1Oeq8iYvr7tOzi3zReJiNiss9iT3ndA3bv2DEhGEwZcQ/edit?usp=sharing">Workshop Day 2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/presentation/d/12xRbyYcthcle-Td6uL9qzzjI5WkWSfozh7iaXQNFmoE/edit?usp=sharing">Workshop Day 3&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/presentation/d/12LMslY_fMwQ6A5h3vJ2SnOmPThfJwuNvANsT0F3EtMs/edit?usp=sharing">Workshop Day 4&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Cognition and BOLD Signal Related to Early Stages of Puberty</title><link>/projects/ncog-pub-rome/</link><pubDate>Tue, 25 Jun 2019 00:00:00 +0000</pubDate><guid>/projects/ncog-pub-rome/</guid><description>&lt;p>This is the project I presented at the OHBM 2019 conference in Rome - &lt;a href="https://www.slideshare.net/slideshow/embed_code/key/uJK54jXcatbLwL">&amp;ldquo;Differences in Cognition and BOLD Signal Related to Early Stages of Puberty&amp;rdquo;&lt;/a>.&lt;/p>
&lt;p>&lt;img src="/images/projects/ohbm_poster_2019.jpg" alt="">&lt;/p></description></item><item><title>ABCD Brain Analysis Tool</title><link>/projects/abcd-bat/</link><pubDate>Wed, 05 Jun 2019 00:00:00 +0000</pubDate><guid>/projects/abcd-bat/</guid><description>&lt;p>This is the abstract I submitted to OHBM 2020 for the ABCD Brain Analysis Tool. The GUI was built under the guidance of the great Bader Chaarani. We thought about naming it something punny after Bader - i.e. The ABCD BAD Tool - but we couldn&amp;rsquo;t think of anything appropriate for submission.&lt;/p>
&lt;p>Anyways, here&amp;rsquo;s the abstract:&lt;/p>
&lt;p>&lt;strong>Authors:&lt;/strong> Philip Nguyen, Hugh Garavan PhD, Alexandra Potter PhD, Bader Chaarani PhD&lt;/p>
&lt;p>&lt;strong>Introduction:&lt;/strong>
Given the computational and analytical expertise required to study large MRI datasets such as that generated by the Adolescent Brain Cognitive Development (ABCD) study, we created the ABCD Brain Analysis Tool (BAT) to allow researchers from various academic backgrounds to utilize fMRI data in their research. Here, we introduce a fast and intuitive graphical user interface (GUI) that automates design matrix creation for PALM, parametric and non-parametric analyses, and t-statistic and effect-size map generation using ABCD voxel and vertex data.&lt;/p>
&lt;p>&lt;strong>Methods:&lt;/strong>
The ABCD imaging and behavioral data were collected from 11,880 9-10 year olds across 21 sites in the United States. Functional MRI data have been optimized, harmonized, and processed for all sites, which include the Monetary Incentive Delay (MID), the Stop Signal (SST), and the Emotional N-Back (N-BACK) tasks. These tasks measure reward processing, response inhibition, and working memory, respectively. The data are provided in the application package as a raw 4-D Matlab matrix.&lt;/p>
&lt;p>Matlab’s AppDesigner module was used to create the interface. Users are only required to provide a list of subjects and a measure of interest. The imaging and demographic variables are then matched and subsetted with the user-provided inputs, and a design matrix is created with age, sex, puberty, race, handedness, highest combined income, and site as default covariates. Users can run between-group or regression analyses on voxel and/or vertex data using parametric or non-parametric algorithms. Cohen&amp;rsquo;s d effect sizes can also be computed for each voxel/vertex as the mean of the fMRI BOLD betas of the contrast divided by the standard deviation of the betas. The computed images are then projected on a Freesurfer or an MNI template.&lt;/p>
&lt;p>&lt;strong>Results:&lt;/strong>
The BAT’s interface is shown in figure 1. The workflow is as follows: First the user imports a list of subjects and a measure of interest using the “Import PGUIDS &amp;amp; Independent Variable” button (fig. 1-1). Users then select an fMRI contrast and covariates using the available options (fig. 1-2). Afterwards the user generates a design matrix and can preview it by clicking the “Create Design Matrix” button (fig. 1-3 &amp;amp; 1-4). Optionally, the user can save a local copy of the design matrix as a text file (fig. 1-5). The last step involves setting the parameters for the type of analysis the user would like to perform (fig. 1-6). An example output for an analysis comparing the 2-back vs. fixation contrast of the N-Back on the cortical level between males and females is shown in figure 2. Here, Cohen’s d maps are thresholded at ≥ 0.2 where red areas represent bigger BOLD activation in males and blue areas represent bigger BOLD activation in females.&lt;/p>
&lt;p>&lt;img src="/images/projects/fig1.png" alt="">&lt;/p>
&lt;p>&lt;img src="/images/projects/fig2.png" alt="">&lt;/p>
&lt;p>The BAT performed better than manual preparation in terms of data preparation and setting-up design matrices, a nontrivial task in neuroimaging analysis (up to five minutes using the BAT versus thirty minutes or longer via manual preparation). Similarly, since the imaging data are available in the package in a raw format, the BAT performed faster at running permutation analyses compared to standard software. For instance, on the same computer with medium memory and processor specifications, running 1000 permutations on 2000 subjects takes up to thirty minutes using the BAT and more than an hour using standard neuroimaging software such as FSL or PALM.&lt;/p>
&lt;p>&lt;strong>Conclusion:&lt;/strong>
The ABCD BAT is a simple graphical user interface that enables researchers without expertise in neuroimaging and large data science to prepare, run and visualize whole-brain fMRI analyses automatically. Researchers are able to generate faster results with the BAT than if they had used the standard neuroimaging software that are currently available.&lt;/p></description></item><item><title>ABCD Data Dictionary</title><link>/projects/abcd-dd/</link><pubDate>Wed, 05 Jun 2019 00:00:00 +0000</pubDate><guid>/projects/abcd-dd/</guid><description>&lt;p>I built a searchable data dictionary and ABCD release notes all in one place, which addresses some of the navigational issues I had with NDA&amp;rsquo;s data dictionary.&lt;/p>
&lt;p>Unfortunately I can&amp;rsquo;t show the app&amp;rsquo;s source code due to confidentiality, but this is a GIF demonstrating what it does.&lt;/p>
&lt;p>&lt;img src="/images/projects/data-dict.gif" alt="">&lt;/p>
&lt;p>Built using R and Shiny. Data pulled from &lt;a href="https://nda.nih.gov/">NDA website&lt;/a>.&lt;/p></description></item><item><title>ABCD Database Builder</title><link>/projects/abcd-db-builder/</link><pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate><guid>/projects/abcd-db-builder/</guid><description>&lt;p>This is an app I built in Shiny to help ABCD Researchers download ABCD datasets (Unfortunately I can&amp;rsquo;t show the app due to confidentiality, but the source code is &lt;a href="https://github.com/nguyenhphilip/ABCD_Database_Builder">here.&lt;/a>)&lt;/p>
&lt;p>Here is a GIF demonstrating some of the app&amp;rsquo;s functionality.&lt;/p>
&lt;p>You can select which spreadsheet(s) you want to download, specific variables from the spreadsheets, which visits to include, and what to name your file.&lt;/p>
&lt;p>&lt;img src="/images/projects/abcd-db-build.gif" alt="">&lt;/p>
&lt;p>Developed with R and Shiny.&lt;/p></description></item></channel></rss>