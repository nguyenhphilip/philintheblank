
{
    "version": "https://jsonfeed.org/version/1.1",
    "title": "Projects on Phil in the Blank",
    "description": "Recent content in Projects on Phil in the Blank",
    "home_page_url": "/",
    "feed_url": "/projects/index.json",
    "language": "en-us",
    "items": [
        {
            "title": "Predicting mental health and substance use in adulthood from resilience in adolescence",
            "date_published": "2022-12-19T00:00:00Z",
            "date_modified": "2022-12-19T00:00:00Z",
            "id": "/projects/resilience-imagen/",
            "url": "/projects/resilience-imagen/",
            "content_html": "\u003ch2 id=\"project-summary-bayes-mediation-lots-of-data\"\u003eProject Summary: Bayes. Mediation. Lots of data.\u003c/h2\u003e\n\u003cp\u003eIn this study I use data from the \u003ca href=\"https://imagen-project.org/\"\u003eIMAGEN Study\u003c/a\u003e (initial n=2315) to determine if mental health and drug use in adulthood (~age 23) can be predicted by resilience in early adolescence (~age 14). Adolescents are classified into one of four groups following \u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/27079174\"\u003eBurt et al’s\u003c/a\u003e operationalization of resilience as the interaction between competence and adversity. In addition, I look at whether structural brain differences at age 14 in brain regions identified by Burt mediate any of the differences observed in mental health outcomes and alcohol use at age 23.\u003c/p\u003e\n\u003cp\u003eTo summarize, I find that there are significant differences in mental health outcomes and recent drug use at age 23 between the four groups. Additionally, the gray matter volume differences identified between the groups at age 14 did not mediate any of the differences in mental health or drug use. These results suggest that developmental features of early resilience such as social skills interact with adversity in a positive way and are predictive of numerous outcomes in adulthood. This corroborates with findings by (Roisman et al. 2004) that suggest the importance of developmentally salient tasks in predicting adult success.\u003c/p\u003e\n\u003cp\u003eView the rest of the interact report \u003ca href=\"https://nguyenhphilip.github.io/resilience-imagen/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/tds-contrast-1.png\" alt=\"\"\u003e\u003c/p\u003e\n"
        },
        {
            "title": "Exploring Mental Health Networks",
            "date_published": "2021-12-28T00:00:00Z",
            "date_modified": "2021-12-28T00:00:00Z",
            "id": "/projects/pocs-mh-nets/",
            "url": "/projects/pocs-mh-nets/",
            "content_html": "\u003cp\u003eThis is a brief project where I look at mental health symptom networks in a large sample of youth from the \u003ca href=\"https://abcdstudy.org/\"\u003eABCD Study\u003c/a\u003e for the course \u003ca href=\"https://pdodds.w3.uvm.edu/teaching/courses/2021-2022principles-of-complex-systems/\"\u003ePrinciples of Complex Systems\u003c/a\u003e. I was largely inspired by the work being done by \u003ca href=\"https://onlinelibrary.wiley.com/doi/full/10.1002/wps.20375\"\u003eDenny Borsboom\u003c/a\u003e and \u003ca href=\"http://psychosystems.org/NetworkSchool\"\u003ecompany\u003c/a\u003e using network analysis to study mental health. There is still a lot of work to do, but the report for the class can be found \u003ca href=\"https://github.com/nguyenhphilip/pocs-mh-nets/blob/main/CSYS_Mental_Health_Networks.pdf\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCheck out \u003ca href=\"https://psyarxiv.com/f68ej/\"\u003ethis awesome paper\u003c/a\u003e for more on applying complex systems thinking to psychopathology!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/mh-net1.png\" alt=\"\"\u003e\u003c/p\u003e\n"
        },
        {
            "title": "Bayesian PCA",
            "date_published": "2021-12-27T00:00:00Z",
            "date_modified": "2021-12-27T00:00:00Z",
            "id": "/projects/bayesian-pca/",
            "url": "/projects/bayesian-pca/",
            "content_html": "\u003cp\u003eThis is a project I worked on for my Bayesian Statistics course. My partner and I explored Bayesian Principal Component Analysis (BPCA) as an extension of PCA and PPCA on MNIST and an ecological dataset. I learned quite a bit about PCA in the process! The final report can be found \u003ca href=\"https://github.com/nguyenhphilip/stat330project/blob/main/STAT_330_Preprint_draft.pdf\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/bpca1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/bpca2.png\" alt=\"\"\u003e\u003c/p\u003e\n"
        },
        {
            "title": "Character Space \u0026 TV Tropes",
            "date_published": "2021-05-05T00:00:00Z",
            "date_modified": "2021-05-05T00:00:00Z",
            "id": "/projects/character-space/",
            "url": "/projects/character-space/",
            "content_html": "\u003cp\u003eThis is an exploratory project I worked on with another graduate student in Data Science II. We explore the space of personality in fictional characters using data from \u003ca href=\"https://openpsychometrics.org/tests/characters/\"\u003eOpenPsychometrics\u003c/a\u003e. The bulk of the project was spent on probing and describing the space using dimensionality reduction with SVD. A link to the full work is \u003ca href=\"CSYS_387_Final_Paper.pdf\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/cspace1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/cspace2.png\" alt=\"\"\u003e\u003c/p\u003e\n"
        },
        {
            "title": "Spotify Top 100 Songs Analysis",
            "date_published": "2020-12-19T00:00:00Z",
            "date_modified": "2020-12-19T00:00:00Z",
            "id": "/projects/spotify-top100/",
            "url": "/projects/spotify-top100/",
            "content_html": "\u003cp\u003eI\u0026rsquo;ve been an avid Spotify user since my younger high-school gaming days (hipster flex), but Spotify only recently started releasing yearly music reviews that summarize the music you\u0026rsquo;ve listened to throughout that year. After seeing mine for 2020, I wondered how much my music taste had changed over time, especially given the 2020 coronavirus pandemic. Thankfully there\u0026rsquo;s a R package called \u003ccode\u003espotifyr\u003c/code\u003e that provides intuitive wrapper functions that allows one to analyze practically the entirety of Spotify music data! What follows is a quick, informal dive into this. (For getting setup, I suggest checking out the Github repo \u003ca href=\"https://github.com/charlie86/spotifyr\"\u003ehere\u003c/a\u003e and this tutorial \u003ca href=\"https://msmith7161.github.io/what-is-speechiness/\"\u003ehere\u003c/a\u003e.)\u003c/p\u003e\n\u003ch1 id=\"the-data-and-song-features\"\u003eThe Data and Song Features\u003c/h1\u003e\n\u003cp\u003eTo answer my question, I\u0026rsquo;m using my \u0026ldquo;Top 100 songs of the Year\u0026rdquo; from 2016-2020 (500 songs), looking specifically at nine features associated with each song (pretty neat that Spotify does this!). I knew I\u0026rsquo;d been listening to more instrumental, acoustic music since I started learning to play guitar, so I was curious about what the data showed. The features, from their \u003ca href=\"https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/\"\u003eAPI\u003c/a\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eacousticness\u003c/strong\u003e: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edanceability\u003c/strong\u003e: How suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eenergy\u003c/strong\u003e: A measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003einstrumentalness\u003c/strong\u003e: Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eliveness\u003c/strong\u003e: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eloudness\u003c/strong\u003e: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003espeechiness\u003c/strong\u003e: Detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003etempo\u003c/strong\u003e: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003evalence\u003c/strong\u003e: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"uh-oh\"\u003eUh Oh\u003c/h1\u003e\n\u003cp\u003eWellp, looking at the different facets, looks like acousticness and instrumentalness has increased, just as suspected. But valence (positive emotion) has decreased, hitting a low in 2020! Whether or not I was actually sadder on average each day of 2020 is another story\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/pressure-1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s a closer look via some bar plots. The general trend is that everything has decreased for each feature except acousticness and instrumentalness. Loudness has decreased though, which maybe just means I\u0026rsquo;m getting older\u0026hellip; or more sensitive to loud music. But it makes sense if you think about the increase in acousticness.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/barplot-1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAnd numbers. Voilà, une table.\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eFeatures\u003c/th\u003e\n          \u003cth\u003e2016\u003c/th\u003e\n          \u003cth\u003e2017\u003c/th\u003e\n          \u003cth\u003e2018\u003c/th\u003e\n          \u003cth\u003e2019\u003c/th\u003e\n          \u003cth\u003e2020\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eacousticness\u003c/td\u003e\n          \u003ctd\u003e0.24\u003c/td\u003e\n          \u003ctd\u003e0.28\u003c/td\u003e\n          \u003ctd\u003e0.39\u003c/td\u003e\n          \u003ctd\u003e0.48\u003c/td\u003e\n          \u003ctd\u003e0.56\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003edanceability\u003c/td\u003e\n          \u003ctd\u003e0.59\u003c/td\u003e\n          \u003ctd\u003e0.62\u003c/td\u003e\n          \u003ctd\u003e0.56\u003c/td\u003e\n          \u003ctd\u003e0.53\u003c/td\u003e\n          \u003ctd\u003e0.49\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eenergy\u003c/td\u003e\n          \u003ctd\u003e0.63\u003c/td\u003e\n          \u003ctd\u003e0.62\u003c/td\u003e\n          \u003ctd\u003e0.52\u003c/td\u003e\n          \u003ctd\u003e0.49\u003c/td\u003e\n          \u003ctd\u003e0.42\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003einstrumentalness\u003c/td\u003e\n          \u003ctd\u003e0.12\u003c/td\u003e\n          \u003ctd\u003e0.11\u003c/td\u003e\n          \u003ctd\u003e0.18\u003c/td\u003e\n          \u003ctd\u003e0.24\u003c/td\u003e\n          \u003ctd\u003e0.52\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eliveness\u003c/td\u003e\n          \u003ctd\u003e0.18\u003c/td\u003e\n          \u003ctd\u003e0.15\u003c/td\u003e\n          \u003ctd\u003e0.17\u003c/td\u003e\n          \u003ctd\u003e0.15\u003c/td\u003e\n          \u003ctd\u003e0.14\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eloudness\u003c/td\u003e\n          \u003ctd\u003e-7.41\u003c/td\u003e\n          \u003ctd\u003e-8.07\u003c/td\u003e\n          \u003ctd\u003e-10.11\u003c/td\u003e\n          \u003ctd\u003e-11.85\u003c/td\u003e\n          \u003ctd\u003e-13.37\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003espeechiness\u003c/td\u003e\n          \u003ctd\u003e0.09\u003c/td\u003e\n          \u003ctd\u003e0.07\u003c/td\u003e\n          \u003ctd\u003e0.06\u003c/td\u003e\n          \u003ctd\u003e0.04\u003c/td\u003e\n          \u003ctd\u003e0.04\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003etempo\u003c/td\u003e\n          \u003ctd\u003e122.08\u003c/td\u003e\n          \u003ctd\u003e123.69\u003c/td\u003e\n          \u003ctd\u003e121.12\u003c/td\u003e\n          \u003ctd\u003e121.26\u003c/td\u003e\n          \u003ctd\u003e116.32\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003evalence\u003c/td\u003e\n          \u003ctd\u003e0.45\u003c/td\u003e\n          \u003ctd\u003e0.48\u003c/td\u003e\n          \u003ctd\u003e0.41\u003c/td\u003e\n          \u003ctd\u003e0.42\u003c/td\u003e\n          \u003ctd\u003e0.31\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"inconclusive-conclusions\"\u003eInconclusive Conclusions\u003c/h1\u003e\n\u003cp\u003eI\u0026rsquo;d say from personal experience that this trend toward increased acousticness and instrumentalness is a general feature of widened musical taste. I imagine it\u0026rsquo;s also associated with more bouts of work and study sessions, especially in 2020 given I started graduate school. But I also discovered more music of an acoustic, instrumental nature (Olafur Arnalds, City of the Sun, Max Richter) which I played HEAVILY on repeat while slacklining and rollerblading, two new hobbies I picked up over the summer. My love of fingerpicking on guitar probably has an influence on that too.\u003c/p\u003e\n\u003cp\u003eAs for the valence, well, let\u0026rsquo;s find out next year if that changes. I don\u0026rsquo;t actually feel like my subjective experience of days are all too different from the previous years, although some days are definitely lonelier. In moments like those I often turn to music. What the data doesn\u0026rsquo;t capture is the upswing in mood I feel from that form of connection. And don\u0026rsquo;t worry, I\u0026rsquo;m still dancing in my kitchen.\u003c/p\u003e\n"
        },
        {
            "title": "Data Science I Final Project: TV Tropes",
            "date_published": "2020-12-11T00:00:00Z",
            "date_modified": "2020-12-11T00:00:00Z",
            "id": "/projects/tv-tropes/",
            "url": "/projects/tv-tropes/",
            "content_html": "\u003cp\u003eThis is a project I worked on with another graduate student in Data Science I. We open-sourced the code here: \u003ca href=\"https://github.com/jwzimmer/tv-tropes\"\u003eTV Tropes Data Science I Project\u003c/a\u003e. The project involved web scraping a large chunk of webpages from TV Tropes,  network analysis, and data science!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/top_100nodes_6communities.png\" alt=\"\"\u003e\u003c/p\u003e\n"
        },
        {
            "title": "Life After: A #NET-COVID Survey of Beliefs",
            "date_published": "2020-04-27T00:00:00Z",
            "date_modified": "2020-04-27T00:00:00Z",
            "id": "/projects/life-after/",
            "url": "/projects/life-after/",
            "content_html": "\u003cp\u003eThe \u003ca href=\"https://sites.google.com/view/life-after-covid19/english\"\u003eLife After\u003c/a\u003e project is a survey of beliefs about what people believe the world will look like after the COVID-19 pandemic ends. I created \u003ca href=\"https://phil-nguyen.shinyapps.io/life-after/\"\u003ean interactive website\u003c/a\u003e to show the results of our analyses using R and Shiny.\u003c/p\u003e\n\u003cp\u003eThe motivation:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCOVID-19 is one of the largest pandemics of the century. It will have lasting effects on society around the globe. There has been a great deal of speculation about what those effects may be. But, there has yet to be an assessment of what individuals believe the effects will be. This study aims to create a picture of the world after the pandemic, from the perspectives of individuals across the globe. The survey collects anonymous information about socio-demographic and individual characteristics, and asks questions regarding your beliefs about the future behaviors of yourself and others.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis project was created as part of the \u003ca href=\"http://www.combine.umd.edu/network-epidemiology/\"\u003eNetwork Epidemiology workshop\u003c/a\u003e presented by the University of Maryland COMBINE program in collaboration with the University of Vermont Complex Systems Center. We had two weeks to design the study, collect responses, and analyze the data.\u003c/p\u003e\n\u003cp\u003eAll in all, it was a fun project and I learned a lot about Shiny app development, making GIFs, and survey research in general. I am most grateful for the friendships I have developed out of this. Thanks Sam, José, and Rory!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/life-after.png\" alt=\"\"\u003e\u003c/p\u003e\n"
        },
        {
            "title": "Prematurity, Mental Health, and Cognition",
            "date_published": "2019-07-31T00:00:00Z",
            "date_modified": "2019-07-31T00:00:00Z",
            "id": "/projects/prematurity-analysis/",
            "url": "/projects/prematurity-analysis/",
            "content_html": "\u003cp\u003eThis is a project examining the effect of premature birth on mental health and neurocognition in a large sample of youth from the \u003ca href=\"abcdstudy.org\"\u003eABCD Study\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBelow is an interactive HTML page using R Markdown for the analyses and visualization. The source code is posted \u003ca href=\"https://github.com/nguyenhphilip/interactive_analysis_prem\"\u003ehere\u003c/a\u003e and there is an interactive Rmarkdown page \u003ca href=\"premat\"\u003ehere\u003c/a\u003e as well.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/premat-bruno-img.png\" alt=\"\"\u003e\u003c/p\u003e\n"
        },
        {
            "title": "Intro to R Workshop",
            "date_published": "2019-07-10T00:00:00Z",
            "date_modified": "2019-07-10T00:00:00Z",
            "id": "/projects/intro-r-workshop/",
            "url": "/projects/intro-r-workshop/",
            "content_html": "\u003cp\u003eThese are some slides for an introductory R and Tidyverse workshop I lead for undergraduates and graduate students working in our lab. We ran 4 workshops total:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDay 1: Setup R, RStudio, motivations for learning R\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.google.com/presentation/d/1Oeq8iYvr7tOzi3zReJiNiss9iT3ndA3bv2DEhGEwZcQ/edit?usp=sharing\"\u003eWorkshop Day 2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.google.com/presentation/d/12xRbyYcthcle-Td6uL9qzzjI5WkWSfozh7iaXQNFmoE/edit?usp=sharing\"\u003eWorkshop Day 3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.google.com/presentation/d/12LMslY_fMwQ6A5h3vJ2SnOmPThfJwuNvANsT0F3EtMs/edit?usp=sharing\"\u003eWorkshop Day 4\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n"
        },
        {
            "title": "Cognition and BOLD Signal Related to Early Stages of Puberty",
            "date_published": "2019-06-25T00:00:00Z",
            "date_modified": "2019-06-25T00:00:00Z",
            "id": "/projects/ncog-pub-rome/",
            "url": "/projects/ncog-pub-rome/",
            "content_html": "\u003cp\u003eThis is the project I presented at the OHBM 2019 conference in Rome - \u003ca href=\"https://www.slideshare.net/slideshow/embed_code/key/uJK54jXcatbLwL\"\u003e\u0026ldquo;Differences in Cognition and BOLD Signal Related to Early Stages of Puberty\u0026rdquo;\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/ohbm_poster_2019.jpg\" alt=\"\"\u003e\u003c/p\u003e\n"
        },
        {
            "title": "ABCD Brain Analysis Tool",
            "date_published": "2019-06-05T00:00:00Z",
            "date_modified": "2019-06-05T00:00:00Z",
            "id": "/projects/abcd-bat/",
            "url": "/projects/abcd-bat/",
            "content_html": "\u003cp\u003eThis is the abstract I submitted to OHBM 2020 for the ABCD Brain Analysis Tool. The GUI was built under the guidance of the great Bader Chaarani. We thought about naming it something punny after Bader - i.e. The ABCD BAD Tool - but we couldn\u0026rsquo;t think of anything appropriate for submission.\u003c/p\u003e\n\u003cp\u003eAnyways, here\u0026rsquo;s the abstract:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAuthors:\u003c/strong\u003e Philip Nguyen, Hugh Garavan PhD, Alexandra Potter PhD, Bader Chaarani PhD\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\nGiven the computational and analytical expertise required to study large MRI datasets such as that generated by the Adolescent Brain Cognitive Development (ABCD) study, we created the ABCD Brain Analysis Tool (BAT) to allow researchers from various academic backgrounds to utilize fMRI data in their research. Here, we introduce a fast and intuitive graphical user interface (GUI) that automates design matrix creation for PALM, parametric and non-parametric analyses, and t-statistic and effect-size map generation using ABCD voxel and vertex data.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMethods:\u003c/strong\u003e\nThe ABCD imaging and behavioral data were collected from 11,880 9-10 year olds across 21 sites in the United States. Functional MRI data have been optimized, harmonized, and processed for all sites, which include the Monetary Incentive Delay (MID), the Stop Signal (SST), and the Emotional N-Back (N-BACK) tasks. These tasks measure reward processing, response inhibition, and working memory, respectively. The data are provided in the application package as a raw 4-D Matlab matrix.\u003c/p\u003e\n\u003cp\u003eMatlab’s AppDesigner module was used to create the interface. Users are only required to provide a list of subjects and a measure of interest. The imaging and demographic variables are then matched and subsetted with the user-provided inputs, and a design matrix is created with age, sex, puberty, race, handedness, highest combined income, and site as default covariates. Users can run between-group or regression analyses on voxel and/or vertex data using parametric or non-parametric algorithms. Cohen\u0026rsquo;s d effect sizes can also be computed for each voxel/vertex as the mean of the fMRI BOLD betas of the contrast divided by the standard deviation of the betas. The computed images are then projected on a Freesurfer or an MNI template.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\nThe BAT’s interface is shown in figure 1. The workflow is as follows: First the user imports a list of subjects and a measure of interest using the “Import PGUIDS \u0026amp; Independent Variable” button (fig. 1-1). Users then select an fMRI contrast and covariates using the available options (fig. 1-2). Afterwards the user generates a design matrix and can preview it by clicking the “Create Design Matrix” button (fig. 1-3 \u0026amp; 1-4). Optionally, the user can save a local copy of the design matrix as a text file (fig. 1-5). The last step involves setting the parameters for the type of analysis the user would like to perform (fig. 1-6). An example output for an analysis comparing the 2-back vs. fixation contrast of the N-Back on the cortical level between males and females is shown in figure 2. Here, Cohen’s d maps are thresholded at ≥ 0.2 where red areas represent bigger BOLD activation in males and blue areas represent bigger BOLD activation in females.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/fig1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/fig2.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe BAT performed better than manual preparation in terms of data preparation and setting-up design matrices, a nontrivial task in neuroimaging analysis (up to five minutes using the BAT versus thirty minutes or longer via manual preparation). Similarly, since the imaging data are available in the package in a raw format, the BAT performed faster at running permutation analyses compared to standard software. For instance, on the same computer with medium memory and processor specifications, running 1000 permutations on 2000 subjects takes up to thirty minutes using the BAT and more than an hour using standard neuroimaging software such as FSL or PALM.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eConclusion:\u003c/strong\u003e\nThe ABCD BAT is a simple graphical user interface that enables researchers without expertise in neuroimaging and large data science to prepare, run and visualize whole-brain fMRI analyses automatically. Researchers are able to generate faster results with the BAT than if they had used the standard neuroimaging software that are currently available.\u003c/p\u003e\n"
        },
        {
            "title": "ABCD Data Dictionary",
            "date_published": "2019-06-05T00:00:00Z",
            "date_modified": "2019-06-05T00:00:00Z",
            "id": "/projects/abcd-dd/",
            "url": "/projects/abcd-dd/",
            "content_html": "\u003cp\u003eI built a searchable data dictionary and ABCD release notes all in one place, which addresses some of the navigational issues I had with NDA\u0026rsquo;s data dictionary.\u003c/p\u003e\n\u003cp\u003eUnfortunately I can\u0026rsquo;t show the app\u0026rsquo;s source code due to confidentiality, but this is a GIF demonstrating what it does.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/data-dict.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eBuilt using R and Shiny. Data pulled from \u003ca href=\"https://nda.nih.gov/\"\u003eNDA website\u003c/a\u003e.\u003c/p\u003e\n"
        },
        {
            "title": "ABCD Database Builder",
            "date_published": "2019-01-05T00:00:00Z",
            "date_modified": "2019-01-05T00:00:00Z",
            "id": "/projects/abcd-db-builder/",
            "url": "/projects/abcd-db-builder/",
            "content_html": "\u003cp\u003eThis is an app I built in Shiny to help ABCD Researchers download ABCD datasets (Unfortunately I can\u0026rsquo;t show the app due to confidentiality, but the source code is \u003ca href=\"https://github.com/nguyenhphilip/ABCD_Database_Builder\"\u003ehere.\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eHere is a GIF demonstrating some of the app\u0026rsquo;s functionality.\u003c/p\u003e\n\u003cp\u003eYou can select which spreadsheet(s) you want to download, specific variables from the spreadsheets, which visits to include, and what to name your file.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/abcd-db-build.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eDeveloped with R and Shiny.\u003c/p\u003e\n"
        }
        ]
}
